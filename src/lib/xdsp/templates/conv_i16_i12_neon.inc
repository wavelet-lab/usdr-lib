const uint64x2_t maske = vdupq_n_u64(0x0000fff00000fff0);
const uint64x2_t masko = vdupq_n_u64(0xfff00000fff00000);

const uint8x16_t lk0 = {0x01,0x02,0x03, 0x05,0x06,0x07, 0x09,0x0a,0x0b, 0x0d,0x0e,0x0f, 0x80,0x80,0x80,0x80};
const uint8x16_t lk1 = {0x80,0x80,0x80,0x80, 0x01,0x02,0x03, 0x05,0x06,0x07, 0x09,0x0a,0x0b, 0x0d,0x0e,0x0f};

/*
*  |              (2)              |              (1)              |              (0)              |
*  +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
*  | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 |
*  +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
*
*  | f7  |0| f6  |0| f5  |0| f4  |0| f3  |0| f2  |0| f1  |0| f0  |0|        i0
*  | f15 |0| f14 |0| f13 |0| f12 |0| f11 |0| f10 |0| f9  |0| f8  |0|        i1
*
*  |00000|0| f6  |0|00000|0| f4  |0|00000|0| f2  |0|00000|0| f0  |0|        s0_1
*  |00000| f6  |0|00000|0| f4  |0|00000|0| f2  |0|00000|0| f0  |0|0|        s0_1 << 4
*
*  | f7  |0|00000|0| f5  |0|00000|0| f3  |0|00000|0| f1  |0|00000|0|        s0_2
*
*  | f7  | f6  |0|0| f5  | f4  |0|0| f3  | f2  |0|0| f1  | f0  |0|0|        s0_1|s0_2
*  | f15 | f14 |0|0| f13 | f12 |0|0| f11 | f10 |0|0| f9  | f8  |0|0|        s1_1|s1_2
*  +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
*  | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 |
*  +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
*  | f   e   d   c   b   a   9   8   7   6   5   4   3   2   1   0
*
*  |              (2)              |              (1)              |              (0)              |
*  +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
*  | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 |
*  +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
*  |               | f7  | f6  | f5  | f4  | f3  | f2  | f1  | f0  |        res0
*  | f15 | f14 | f13 | f12 | f11 | f10 | f9  | f8  |               |        res1
*
*/

#define CONVERT_I16_I12_BLOCK(i0, i1, rlow, rmid, rhigh) \
    { \
        uint64x2_t s0 = vreinterpretq_u64_s16(i0); \
        uint64x2_t s1 = vreinterpretq_u64_s16(i1); \
    \
        uint64x2_t s0_1 = vshlq_n_u64(vandq_u64(s0, maske), 4); \
        uint64x2_t s0_2 = vandq_u64(s0, masko); \
        uint8x16_t res0 = vreinterpretq_u8_u64(vorrq_u64(s0_1, s0_2)); \
    \
        uint64x2_t s1_1 = vshlq_n_u64(vandq_u64(s1, maske), 4); \
        uint64x2_t s1_2 = vandq_u64(s1, masko); \
        uint8x16_t res1 = vreinterpretq_u8_u64(vorrq_u64(s1_1, s1_2)); \
    \
        res0 = vqtbl1q_u8(res0, lk0); \
        res1 = vqtbl1q_u8(res1, lk1); \
    \
        rlow  = vget_low_u8(res0); \
        rmid  = vorr_u8(vget_high_u8(res0), vget_low_u8(res1)); \
        rhigh = vget_high_u8(res1); \
    }
